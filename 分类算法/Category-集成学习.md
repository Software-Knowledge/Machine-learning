Category-集成学习
---
1. 集成学习并不是简单地将数据集在多个不同分类器上重复训练，而是对数据集进行扰动
2. 一个分类训练中的错误还可以被下一个分类器进行利用
3. 分类器预测错误的原因是未知的实例与学习的实例的分布有区别，通过扰动，分类器可以学习到更加一般的模型，从而消除单个分类器产生的偏差，而得到更为精准的模型，增强模型的鲁棒性。

# 1. 什么是集成学习法(Emsemble Learning)
1. 集成学习法通过多个分类学习方法聚集一起来提高分类准确率，提高模型的稳定性
2. 通常情况下，一个集成分类器的分类型能要好于单个分类器
3. 集成学习法由训练数据构建一组基分类器(base classifier)，然后通过对每个基分类器的预测进行投票来实现分类。
4. 在构建分类器的过程中，一般有两种集成方法
   1. 一种是使用训练集的不同自己训练得到不同的基分类器
   2. 另一种方法是使用同一个训练集的不同属性子集训练得到不同的基分类器

# 2. 集成学习的基本思想
在原始数据集上构建多个分类器，然后在分类未知样本时聚集它们的预测结果。

# 3. 构建集成分类器的过程描述
> 逻辑结构示意图如下

![](img/Emsemble%20Learning/1.png)

# 4. 构建集成分类器的方法

## 4.1. 通过处理训练数据集
1. 它根据某种抽样分布，通过对原始数据进行再抽样来得到多个训练集然后使用特定的学习算法为每个训练集建立一个分类器。
2. 典型的处理训练数据集的组合方法有装袋(bagging)和提升(boosting)

## 4.2. 通过处理输入特征
1. 在这种方法中，通过选择输入特征的自己来形成每个训练集。一些研究表明，对那些含有大量冗余特征的数据集，这种方法的性能非常好。
2. 随机森林(Random forest)就是一种处理输入特征的组合方法

## 4.3. 通过处理类标号
1. 这种方法适用于类数足够多的情况。通过将类标号随机划分成两个不相交的子集$A_0$和$A_1$，把训练数据变换为二类问题，类标号属于子集$A_0$的训练样本指派到类0，而那些类标号属于子集$A_1$的训练样本指派到类1。
2. 然后使用重新标记过的数据来训练一个基分类器，重复重新标记类和构建模型步骤多次，就得到一组基分类器
3. 当遇到一个检验样本时，使用每个基分类器$C_i$预测它的类标号。
   1. 如果被预测为类0，则所有属于$A_0$的类都得到一票
   2. 如果被预测为类1，则所有属于$A_1$的类都得到一票
4. 最后统计选票，将检验样本指派到得票最高的类。