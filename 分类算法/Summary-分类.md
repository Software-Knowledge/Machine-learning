Summary-分类
---

# 1. 什么是分类问题
1. 分类就是给定一个带有标签的实体的训练集，制定一个将标签分配给测试集中的实体的规则。

## 1.1. 分类问题的特点
1. 可能有一个简单的分隔符(例如，二维直线或一维超平面)，一些分类器显式表示分隔符(例如，直线)，也有部分分类器用隐式的方式完成分隔。
2. 可能存在各种噪声
3. 可能存在数据重叠
4. 不应该被低维的几何直觉所欺骗

## 1.2. 分类问题中的物体表示
1. 每个要分类的对象都表示为一对(x, y)
   1. 其中x是对象的描述
   2. 其他y是相应的标签(本文中假设为二分类)
2. 机器学习分类器的成功或失败通常取决于选择正确的对象描述
   1. 描述的选择也可以看做机器学习问题
   2. 这里需要良好的直觉

## 1.3. 分类问题中的数据类型
1. 数据处理目标：向量化数据(数值形式)
   1. 物理特征
   2. 行为特征
   3. 上下文：目前所在的环境
   4. 历史：过去的足迹
2. 数据表现的形式
   1. 文本和超文本
   2. 邮件
   3. 蛋白质序列
   4. Unix系统调用的顺序
   5. 网络层:图
   6. 图片:识别目标物

## 1.4. 分类问题的训练与验证
> 数据集(标签样本)会被划分为三部分

1. 训练集
2. 验证集
3. 测试集

### 1.4.1. 训练过程概述
> 缺少以下任何一步都可能导致**过拟合**

1. 估计训练集上的参数
2. 调整验证集上的超参数
3. 在训练集上报告结果
4. 整体上来讲就是将训练样本作为学习的数据，结合分类模型或规则得到分类器，然后在测试样本上测试分类器并评估

# 2. 分类效果的评价标准
[Standard-分类评价标准](Standard-分类评价标准.md)

# 3. 分类问题的应用领域(数学意义上)
1. 二分类问题
2. 多分类问题(Eg, 兔子，老虎，牛)
   1. 直接分类
   2. 划分为多个并列的二分类问题合并(Eg, 兔子和虎、虎和牛、兔子和牛)
   3. 划分为多个串行的二分类问题(Eg.非虎，非兔)
3. 非专属类别
4. Rank

# 4. 分类问题示例

## 4.1. 垃圾邮件识别器(Spam Filter)
1. 输入:email
2. 输出:垃圾邮件/不是垃圾邮件
3. 初始化:
   1. 获得很多的样例邮件，已经被标记为spam或者是ham
   2. 想要去预测一个新的未来的邮件是不是垃圾邮件
4. 特点:以下的属性被用来区分邮件是不是垃圾邮件
   1. 词:FREE!
   2. 文本模式:$dd，CAPS
   3. 非文本:发送方

## 4.2. 数字识别
1. 输入:图片或者像素网格
2. 输出:数字0-9
3. 初始化:
   1. 获得很多的样例图片，每一个被标记为一个数字
   2. 想要去预测一个新的未来的数字图片中的数字
4. 特点:以下的属性被用来区分当前图片中的数字
   1. 像素:(6,8)是ON
   2. 形状模式:数字组成、纵横比、循环检测数字
5. 当前状态:和人相似
6. 分类是提前设置好的，有限的，目标明确的。

## 4.3. 其他分类任务的示例
1. 欺诈检测
   1. 输入：帐户活动
   2. 类别：欺诈/无欺诈
2. 网页垃圾邮件检测
   1. 输入：HTML /渲染页面
   2. 类别：垃圾邮件/火腿
3. 语音识别和说话者识别
   1. 输入：波形
   2. 类别：音素或单词
4. 医学诊断
   1. 输入：症状
   2. 类别：疾病
5. 自动作文评分器：
   1. 输入：文档
   2. 输出：成绩
6. 客户服务电子邮件路由和文件夹
7. 社交网络中的链接预测
8. 药物设计中的催化活性

# 5. 常见的分类算法
1. 分类就是通过分析训练集中的数据，为每个类别做出准确的描述或建立分析模型或挖掘出分类规则，然后用这个分类规则对其它数据对象进行分类。

## 5.1. 决策树
1. 获得一个树状的结构，根据特征进行组合

## 5.2. 支持向量机
1. 将低维空间的数据投影到高维空间。

## 5.3. 神经网络
1. 是一个隐模型
2. 传统的神经网络为BP神经网络，基本网络结构为输入层、隐藏层和输出层，节点代表神经元，边代表权重值，对输入值按照权重和偏置计算后将结果传给下一层，通过不断的训练修正权重和偏置。递归神经网络(RNN)、卷积神经网络(CNN)都在神经网络在深度学习上的变种。
3. 神经网络的**训练**主要包括
    1. 前向传输
    2. 反向传播。
4. 神经网络的**结果准确性**与训练集的样本数量和分类质量有关。
5. 神经网络是基于历史数据构建的分析模型，新数据产生时需要动态优化网络的结构和参数。
6. 也可以用作分类算法

### 5.3.1. 卷积神经网路
1. 比较适合分析物体和物体之间的关系。

### 5.3.2. 多层前馈神经网络

### 5.3.3. 循环神经网络

## 5.4. 朴素贝叶斯
[Algorithm-朴素贝叶斯](Algorithm-朴素贝叶斯.md)

## 5.5. Bayes网络

## 5.6. k-最近邻等

# 6. 分类与回归的区别
1. 回归分析是分析一个变量与其他一个(或几个) 变量之间的相关关系的统计方法。
    + 例子:建立一个模型，根据已知的多个变量值来预测其他某个变量值
    + 例子:分析某网上客户的购买数据后发现，购买数据挖掘书籍的用户在同一次购物还会买大数据方面的书籍，使用回归分析的机器学习方法。

# 7. 决策树原理
1. 确定每一步特征空间划分标准时，都同时兼顾由此形成的两个区域，希望划分形成的两个区域所包含的样本点尽可能同时"纯正"。
2. 意义:就是通过我们通过不断选取重要的属性，将样本集从大变小，逐渐变小到每个样本集都属于同一类为止。

## 7.1. 决策树连续属性非监督离散化的常用方法(!)
1. 聚类离散化
2. 等频离散化
3. 等宽离散化

# 8. 例子模型
1. 相亲模型
2. 根据ARPU值预估用户输入

# 9. 连续属性离散化
1. 分类数据有二元属性、标称属性等几种不同类型的离散属性。
2. 一般对数据先进行离散化，在进行数据处理之类的。

## 9.1. 二元属性
1. 二元属性在分支处理时，可以产生两个分支，无须对其数据进行特别的处理。

## 9.2. 标称属性
1. 标称属性可能有多个可能值，针对所使用的决策树算法的不同，标称属性的分类存在两种方式:
    1. 多路划分:ID3、C4.5算法
    2. 二元划分:CART算法，生成二叉树
2. 其中有一类特别的属性为序数属性，其属性的取值是有先后顺序的。对于序数属性的分类，往往要结合实际情况来考虑。

## 9.3. 非监督离散化
1. 非监督离散化不需要使用分类属性值，相对简单，有等宽离散化、等频离散化、聚类等方法
   1. 等宽离散化将属性划分为宽度--致的若千个区间
   2. 等频离散化将属性划分为若千个区间，每个区间的数量相等
   3. 聚类将属性间根据特性划分为不同的簇，以此形式将连续属性离散化
2. 非监督离散化的方法能够完成对连续数据进行离散化的要求，但是相比之下，对连续属性监督离散化很多时候能够产生更好的结果。常用的方法是通过选取极大化区间纯度的临界值来进行划分
   1. C4.5 与CART算法中的连续属性离散化方法均属于监督离散化方法
   2. CART算法使用Gini系数作为区间纯度的度量标准
   3. C4.5算法使用熵作为区间纯度的度量标准
3. 还有数学优化离散化、等深离散化。
4. 常用的是区间分隔来做离散化，要保证分隔点有着明确的相应的含义。

# 10. 过拟合问题(和欠拟合问题)
过拟合就是你用训练样本训练出来的模型，用训练样本来检测的正确率很高，而使用检验样本检验误差很大。
1. 训练误差代表分类方法对于现有训练样本集的拟合程度
2. 泛化误差代表此方法的泛化能力，即对于新的样本数据的分类能力如何
    + 不要过度学习噪声之类的
3. 模型的训练误差比较高，则称此分类模型欠拟合
4. 模型的训练误差低但是泛化误差比较高，则称此分类模型过拟合
5. 对于欠拟合问题，可以通过增加分类属性的数量、选取合适的分类属性等方法，提高模型对于训练样本的拟合程度
6. 最常用的是分为两部分进行对比检测。

## 10.1. 决策树误差曲线
1. 过拟合现象会导致随着决策树的继续增长，尽管训练误差仍在下降，但是泛化误差停止下降，甚至还会提升

![](img\4.png)

1. 横坐标:树的深度或是训练次数
2. 纵坐标:误差
3. 红色曲线:训练样本误差
4. 绿色曲线:测试样本误差
    + 在学习的深度和训练次数的增加过程中，测试样本误差会先下降再上升
    + 所以我们需要在那个关键点处停止学习

## 10.2. 解决过拟合问题
1. 可以使用减少学习的层数来降低过拟合问题。
比如把一开始的树
---
![](img\2.png)

变为
---
![](img\3.png)

2. 可以通过剪枝在相应关键点出停止进行进一步学习。
3. 其他参考方案:
    1. 剪枝方法:保证决策树的复杂程度不断降低。
4. 解决过拟合问题，一.方面要注意数据训练集的质量，选取具有代表性样本的训练样本集。另-方面要避免决策树过度增长，通过限制树的深度来减少数据中的噪声对于决策树构建的影响，一般可以采取剪枝的方法
5. 剪枝是用来缩小决策树的规模，从而降低最终算法的复杂度并提高预测准确度，包括预剪枝和后剪枝两类
6. 预剪枝的思路是提前终止决策树的增长，在形成完全拟合训练样本集的决策树之前就停止树的增长，避免决策树规模过大而产生过拟合
7. 后剪枝策略先让决策树完全生长，之后针对子树进行判断，用叶子结点或者子树中最常用的分支替换子树，以此方式不断改进决策树，直至无法改进为止

## 10.3. 剪枝问题
1. 我们需要通过实验，来决定什么时候进行剪枝
    + 一般是从第三层往后会出现过拟合现象。

### 10.3.1. 错误率降低剪枝
1. 错误率降低剪枝( REP )是后剪枝策略中最简单的算法之一，该算法从叶子结点向上，依次将决策树的所有子树用其样本中最多的类替换，使用一个测试集进行测试，记录下对于决策树的每棵子树剪枝前后的误差数之差选取误差数减少最少的子树进行剪枝，将其用子样本集中最多的类替换按此步骤自底向.上，遍历决策树的所有子树，当发现没有可替换的子树时，即每棵子树剪枝后的误差数都会增多，则剪枝结束
2. REP剪枝方法简单、快速，在数据集较大时效果不错，但由于需要比对模型子树替换前后的预测错误率，因此需要从数据集中划分出单独的测试集，故而当数据集较小时，REP剪 枝策略的效果会有所下降

### 10.3.2. 悲观剪枝
1. 悲观剪枝( PEP )与REP相比，PEP不再需要构建一个单独的测试集。其假设某叶子结点t中有N(t)个样本，其中有e(t)个 被错误分类的样本，则此叶子结点误分类率定义

$$
r(t) = \frac{e(t) + 0.5}{N(t)}
$$

2. 其中0.5为修正因子。对于一棵有着N个叶子结点的子树T，其误分类率计算公式如下

$$
r(T) = \frac{\sum[e(i) + 0.5]}{\sum N(i)} = \frac{\sum[e(i) + \frac{N}{2}]}{\sum N(i)} 
$$

3. 由于修正因子的存在，有时即便子树的误差数要小于剪枝后的误差，仍有可能进行剪枝操作，因为误分类率的计算公式中考虑到了叶子结点树大小(N)的影响

### 10.3.3. 代价复杂度剪枝策略
1. 代价复杂度剪枝策略(CCP)定义了代价与复杂度的概念，代价是指在剪枝过程中因为子树被替换而增加的错分样本，复杂度表示剪枝后减少的叶结点数
2. CCP算法使用a作为衡量代价与复杂度之间关系的值，其计算公式如下

$$
\alpha = \frac{R(t) - R(T_t)}{|N_1|-1}
$$

1. CCP的具体方法为，计算决策树T的每个非叶子结点的a值，每次计算之后剪掉具有最小a值的子树，循环此过程直至只剩下根结点，进行n次剪枝，生成n个决策树，从这n个决策树中根据真实误差估计选择最佳决策树


# 11. 分类效果评价
1. 对于一般分类问题，有训练误差、泛化误差、准确率、错误率等指标
2. 对于常见的二分类问题，样本只有两种分类结果，将其定义为正例与反例
3. 那么在进行分类时，对于一个样本，可能出现的分类情况共有四种:
   1. 样本为正例，被分类为正例，称为真正类(TP)
   2. 样本为正例，被分类为反例，称为假反类(FN)
   3. 样本为反例，被分类为正例，称为假正类(FP)
   4. 样本为反例，被分类为反例，称为真反类(TN)
4. 具体的其他项目指标:
    1. 查全率:是否查的比较全。查全率和准确率往往相互制约，因为查全率高就容易导致准确率的降低。
    2. 准确率(accuracy):分类模型正确分类的样本数(包括正例与反例)与样本总数的比值
    3. 精确率(precision):模型正确分类的正例样本数与总的正例样本总数(即正确分类的正例样本数目与错误分类的正确样本数目之和)的比值
    4. 召回率(recall，也称为查全率):模型分类正确的正例样本数与分类正确的样本总数(分类正确的正例和分类正确的反例之和)的比值

$$
\begin{array}{l}
   accuracy = \frac{TP + TN}{TP + FN + FP + TN} \\
   precision = \frac{TP}{TP + FP} \\
   recall = \frac{TP}{TP + FN} \\
\end{array}
$$

1. F值:真正在决策分析时，先看F值，在F值高一点的情况下，再看查全率等，比如看病会牺牲一定的准确率来保证查全率。
2. F值为准确率和召回率的调和平均

$$
F = \frac{(\alpha^2+1)*accuracy *recall}{\alpha^2(accuracy + recall)}
$$

3. 其中$\alpha$为调和参数值，当$\alpha$取值为1时，F值就是最常见的$F_1$值

$$
F_! = \frac{2*accuracy *recall}{accuracy + recall}
$$

## 11.1. 受试者工作曲线(ROC)曲线

1. 受试者工作特征曲线 (ROC)曲线也是一种常 用的综合评价指标。假设检验集中共有20个样本，每个样本为正类或反类，根据分类算法模型可以得出每个样本属于正类的概率，将样本按照此概率由高到低排列。
2. 曲线如下:
    1. 横坐标表示:假正率。
    2. 纵坐标表示:真正率。
    3. 我想要真正率高，假正率低，也就是想要ROC曲线更加高一点比较好。
    4. 做多次实验，可以通过比较曲线下面积来看，一般越高的曲线，噪声会更多。

![](img\12.png)

## 11.2. 提高相应的比率的方法

### 11.2.1. 保留法
保留法将样本集按照定比例划分为训练集与检验集两个集合，两个集合中样本随机分配且不重叠。对于比例的确定，一般情况下，训练集会大于检验集，例如训练集占70%，检验集占30%，具体比例可结合实际情况进行判定。

### 11.2.2. 蒙特卡洛交叉验证(重复随机二次采样验证)
1. 蒙特卡洛交叉验证，也称重复随机二次采样验证，这种验证方法随机将数据集划分为训练集与检验集，使用检验集检验训练集训练的模型效果，多次重复此过程取平均值作为模型好坏的评价标准。蒙特卡洛交叉验证法也 可看作是多次进行保留法。
2. 多次训练，保留比较好的一颗树。

### 11.2.3. K折交叉验证法
1. k折交叉验证法将样本集随机地划分为k个大小相等的子集，在每一轮交叉验证中，选择一个子集作为检验集，其余子集作为训练集，重复k轮，保证每一个子集都作为检验集出现，用K轮检验结果取平均值作为模型好坏的评价标准。最常用的k折交叉验证法为**十折**交叉验证。
2. 每一次，选择一个子集来作为测试，选择其余子集作为训练集，之后取均值，这样就极大的降低了噪声的影响。

### 11.2.4. 留一法
1. 留一法指每次检验集中只包含一个样本的交叉验证方法。

### 11.2.5. 留p法
1. 留p法是每次使用p个样本作为检验集的交叉验证方法

### 11.2.6. 自助法
1. 自助法是统计学中的一种有放回均匀抽样方法，即从一个大小为𝑛的样本数 据集𝑆中构建一个大小为𝑛′的训练样本集St需要进行𝑛′次抽取，每次均可能 抽取到n个样本中的任何一个。𝑛′次抽取之后，剩余的未被抽取到的样本成 为检验集

# 12. 集成学习
1. 引入:多个评委为一个东西进行评选。
2. 集成学习(Ensemble learning)是机器学习中近年来的一大热门领域。其中的集成方法是用多种学习方法的组合来获取比原方法更优的结果。
3. 使用于组合的算法是弱学习算法，即分类正确率仅比随机猜测略高的学习算法，但是组合之后的效果仍可能高于强学习算法，即集成之后的算法准确率和效率都很高

![](img\13.png)

## 12.1. 装袋法
1. 装袋法(Bagging)又称为Bootstrap Aggregating,其原理是通过组合多个训练集 的分类结果来提升分类效果。
2. 装袋法由于多次采样，每个样本被选中的概率相同，因此噪声数据的影响下降，所以装袋法太容易受到过拟合的影响
3. 使用sklearn库实现的决策树装袋法提升分类效果。其中X和Y分别是鸢尾花 (iris)数据集中的自变量(花的特征)和因变量(花的类别)

```python
from sklearn.model_selection import KFold 
from sklearn.model_selection import cross_val_score 
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import datasets #加载iris数据集
iris = datasets.load_iris()
X = iris.data
Y = iris.target
#分类器及交叉验证
seed = 42
kfold = KFold(n_splits=10,random_state=seed)
cart = DecisionTreeClassifier(criterion='gini',max_depth=2)
cart = cart.fit(X, Y)
result = cross_val_score(cart, X, Y, cv=kfold)
print("CART树结果：",result.mean()) model=BaggingClassifier(base_estimator=cart,n_estimators=100, random_state=seed)
result = cross_val_score(model, X, Y, cv=kfold)
print("装袋法提升后结果：",result.mean())
# CART树结果： 0.933333333333 
# 装袋法提升后结果：0.946666666667
```
4. 总结:可以看到装袋法对模型结果有一定提升。当然，提升程度与原模型的结构和数据质量有关。如果分类回归树的树高度设置为3或5，原算法本身的效果就会比较好，装袋法就没有提升空间.

## 12.2. 提升法
1. 提升法(Boosting)与装袋法相比每次的训练样本均为同一组，并且引入了**权重**的概念，给每个单独的训练样本都会分配个相同的初始权重。然后进行T轮训练，每一轮中使用一个分类方法训练出一个分类模型，使用此分类模型对所有样本进行分类并更新所有样本的权重:分类正确的样本权重降低，分类错误的样本权重增加，从而达到更改样本分布的目的。由此可知，每一轮训练后，都会生成一个分类模型，而每次生成的这个分类模型都会更加注意在之前分类错误的样本，从而提高样本分类的准确率。对于新的样本 ，将T轮训练出的T个分类模型得出的预测结果加权平均，即可得出最终的预测结果。
2. 就是权重低的会被加速进行学习。
3. 多轮分类获得多个分类模型
4. 对新样本预测时采用多伦训练得到的分类模型的预测结果的加权平均值。

### 12.2.1. 示例
1. 基于sklearn库中的提升法分类器对决策树进行优化，提高分类准确率。Python代码如下，其中load_breast_cancer()方法加载乳腺癌数据集，自变量 (细胞核的特征)和因变量(良性、恶性)分别赋给X和Y变量。
```python
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score 
from sklearn.ensemble import AdaBoostClassifier 
from sklearn.tree import DecisionTreeClassifier 
from sklearn import datasets 
dataset_all = datasets.load_breast_cancer()
X = dataset_all.data 
Y = dataset_all.target 
seed = 42
kfold = KFold(n_splits=10, random_state=seed) 
dtree = DecisionTreeClassifier(criterion='gini',max_depth=3) 
dtree = dtree.fit(X, Y) 
result = cross_val_score(dtree, X, Y, cv=kfold) 
print("决策树结果：",result.mean()) 
model = AdaBoostClassifier(base_estimator=dtree, n_estimators=100,random_state=seed) 
result = cross_val_score(model, X, Y, cv=kfold) 
print("提升法改进结果：",result.mean())
# 决策树结果： 0.92969924812
# 提升法改进结果：0.970112781955 
```

## 12.3. GBDT(梯度提升决策树)
1. 梯度提升决策树算法是利用梯度下降的思想，使用损失函数的负梯度在当 前模型的值，作为提升树中残差的近似值，以此来拟合回归决策树。梯度提升决策树的算法过程如下：
    1. 初始化决策树，估计一个使损失函数最小化的常数构建一个只有根节点的树。
    2. 不断提升迭代:
        1. 计算当前模型中损失函数的负梯度值，作为残差的估计值;
        2. 估计回归树中叶子节点的区域，拟合残差的近似值;
        3. 利用线性搜索估计叶子节点区域的值，使损失函数极小化;
        4. 更新决策树。
    3. 经过若干轮的提升法迭代过程之后，输出最终的模型
2. 对于GBDT算法的具体实现，最为出色的是XGBoost树提升系统
```python
import pandas as pd
import xgboost as xgb
df = pd.DataFrame({'x':[1,2,3], 'y':[10,20,30]})
X_train = df.drop('y',axis=1)
Y_train = df['y']
T_train_xgb = xgb.DMatrix(X_train, Y_train)
params = {"objective": "reg:linear", "booster":"gblinear"}
gbm = xgb.train(dtrain=T_train_xgb,params=params)
Y_pred = gbm.predict(xgb.DMatrix(pd.DataFrame({'x':[4,5]}))) print(Y_pred)
```
# 13. 随机森林
1. 随机森林是专为决策树分类器设计的集成方式，是装袋法的一种拓展。随机森林与装袋法采取相同的样本抽取方式。装袋法中的决策树每次从所有属性中选取一个最优的属性作为其分支属性，而随机森林算法每次从所有 属性中随机抽取𝑡个属性，然后从这𝑡个属性中选取一个最优的属性作为其分支属性，这样就使得整个模型的随机性更强，从而使模型的泛化能力更强。而对于参数𝑡的选取，决定了模型的随机性，若样本属性共有M个，𝑡=1意味着随机选择一个属性来作为分支属性，𝑡=属性总数时就变成了装袋法集成方式，通常𝑡的取值为小于log2(M + 1)的最大整数。而随机森林算法使用的弱分类决策树通常为CART算法。
2. 类似装袋法的进行抽取，和装袋法不同:
    1. 装袋法每个树的属性是相同的
    2. 随机森林的每个树的属性也可以使不同的，随机程度更强。
3. 实例代码

```python
from sklearn.model_selection import cross_val_score
from sklearn.datasets import make_blobs 
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.tree import DecisionTreeClassifier
X, y = make_blobs(n_samples=1000, n_features=6, centers=50, random_state=0) 
pyplot.scatter(X[:, 0], X[:, 1], c=y) 
pyplot.show()
```
4. <a href = "https://www.cnblogs.com/yszd/p/9583420.html">最终实现</a>


# 14. 决策树总结
1. ID3算法的分支策略是:信息增益。
2. C4.5算法的分支策略是:信息增益率。
3. CART算法的分支策略是:Gini指标。
4. CHARD算法的分支策略是:卡方。
5. 特征有可能是原始的数据，也可能是衍生的数据。决策树是帮忙来寻找特征。

# 15. 分类算法汇总
<a href ="https://mp.weixin.qq.com/s/hN73sdADY--LnUh7Bo9G5Q">分类算法汇总</a>