**分类算法**
<!-- TOC -->

- [1. 常见的分类算法:](#1-常见的分类算法)
    - [1.1. 决策树](#11-决策树)
    - [1.2. 支持向量机](#12-支持向量机)
    - [1.3. 神经网络](#13-神经网络)
        - [1.3.1. 卷积神经网路](#131-卷积神经网路)
        - [1.3.2. 多层前馈神经网络](#132-多层前馈神经网络)
        - [1.3.3. 循环神经网络](#133-循环神经网络)
    - [1.4. 朴素贝叶斯](#14-朴素贝叶斯)
    - [1.5. Bayes网络](#15-bayes网络)
    - [1.6. k-最近邻等](#16-k-最近邻等)
- [2. 分类与回归的区别](#2-分类与回归的区别)
- [3. 决策树原理](#3-决策树原理)
    - [3.1. 决策树连续属性非监督离散化的常用方法(!)](#31-决策树连续属性非监督离散化的常用方法)
- [4. 例子模型](#4-例子模型)
- [5. 连续属性离散化](#5-连续属性离散化)
    - [5.1. 二元属性](#51-二元属性)
    - [5.2. 标称属性](#52-标称属性)
    - [5.3. 非监督离散化](#53-非监督离散化)
- [6. 过拟合问题(和欠拟合问题)](#6-过拟合问题和欠拟合问题)
    - [6.1. 决策树误差曲线](#61-决策树误差曲线)
    - [6.2. 解决过拟合问题](#62-解决过拟合问题)
    - [6.3. 剪枝问题](#63-剪枝问题)
        - [6.3.1. 错误率降低剪枝](#631-错误率降低剪枝)
        - [6.3.2. 悲观剪枝](#632-悲观剪枝)
        - [6.3.3. 代价复杂度剪枝策略](#633-代价复杂度剪枝策略)
- [7. 分类效果评价](#7-分类效果评价)
    - [7.1. 受试者工作曲线(ROC)曲线](#71-受试者工作曲线roc曲线)
    - [7.2. 提高相应的比率的方法](#72-提高相应的比率的方法)
        - [7.2.1. 保留法](#721-保留法)
        - [7.2.2. 蒙特卡洛交叉验证(重复随机二次采样验证)](#722-蒙特卡洛交叉验证重复随机二次采样验证)
        - [7.2.3. K折交叉验证法](#723-k折交叉验证法)
        - [7.2.4. 留一法](#724-留一法)
        - [7.2.5. 留p法](#725-留p法)
        - [7.2.6. 自助法](#726-自助法)
- [8. 集成学习](#8-集成学习)
    - [8.1. 装袋法](#81-装袋法)
    - [8.2. 提升法](#82-提升法)
        - [8.2.1. 示例](#821-示例)
    - [8.3. GBDT(梯度提升决策树)](#83-gbdt梯度提升决策树)
- [9. 随机森林](#9-随机森林)
- [10. 决策树总结](#10-决策树总结)
- [11. 分类算法汇总](#11-分类算法汇总)

<!-- /TOC -->

# 1. 常见的分类算法:
1. 分类就是通过分析训练集中的数据，为每个类别做出准确的描述或建立分析模型或挖掘出分类规则，然后用这个分类规则对其它数据对象进行分类。
## 1.1. 决策树
1. 获得一个树状的结构，根据特征进行组合

## 1.2. 支持向量机
1. 将低维空间的数据投影到高维空间。

## 1.3. 神经网络
1. 是一个隐模型
2. 传统的神经网络为BP神经网络，基本网络结构为输入层、隐藏层和输出层，节点代表神经元，边代表权重值，对输入值按照权重和偏置计算后将结果传给下一层，通过不断的训练修正权重和偏置。递归神经网络（RNN）、卷积神经网络（CNN）都在神经网络在深度学习上的变种。
3. 神经网络的**训练**主要包括
    1. 前向传输
    2. 反向传播。
4. 神经网络的**结果准确性**与训练集的样本数量和分类质量有关。
5. 神经网络是基于历史数据构建的分析模型，新数据产生时需要动态优化网络的结构和参数。
6. 也可以用作分类算法

### 1.3.1. 卷积神经网路
1. 比较适合分析物体和物体之间的关系。

### 1.3.2. 多层前馈神经网络

### 1.3.3. 循环神经网络

## 1.4. 朴素贝叶斯
1. 用样本训练这个网络，每一条线上都有概率
2. 概率是训练出来的
3. 通过网络进行推理判断，按照概率来推理

## 1.5. Bayes网络

## 1.6. k-最近邻等

# 2. 分类与回归的区别
1. 回归分析是分析一个变量与其他一个（或几个） 变量之间的相关关系的统计方法。
    + 例子:建立一个模型，根据已知的多个变量值来预测其他某个变量值
    + 例子:分析某网上客户的购买数据后发现，购买数据挖掘书籍的用户在同一次购物还会买大数据方面的书籍，使用回归分析的机器学习方法。

# 3. 决策树原理
1. 确定每一步特征空间划分标准时，都同时兼顾由此形成的两个区域，希望划分形成的两个区域所包含的样本点尽可能同时“纯正”。
2. 意义:就是通过我们通过不断选取重要的属性，将样本集从大变小，逐渐变小到每个样本集都属于同一类为止。

## 3.1. 决策树连续属性非监督离散化的常用方法(!)
1. 聚类离散化
2. 等频离散化
3. 等宽离散化

# 4. 例子模型
1. 相亲模型
2. 根据ARPU值预估用户输入

# 5. 连续属性离散化
1. 分类数据有二元属性、标称属性等几种不同类型的离散属性。
2. 一般对数据先进行离散化，在进行数据处理之类的。

## 5.1. 二元属性
1. 二元属性在分支处理时，可以产生两个分支，无须对其数据进行特别的处理。

## 5.2. 标称属性
1. 标称属性可能有多个可能值，针对所使用的决策树算法的不同，标称属性的分类存在两种方式:
    1. 多路划分:ID3、C4.5算法
    2. 二元划分:CART算法，生成二叉树
2. 其中有一类特别的属性为序数属性，其属性的取值是有先后顺序的。对于序数属性的分类，往往要结合实际情况来考虑。

## 5.3. 非监督离散化

![](img\1.png)

1. 还有数学优化离散化、等深离散化。
2. 常用的是区间分隔来做离散化，要保证分隔点有着明确的相应的含义。

# 6. 过拟合问题(和欠拟合问题)
过拟合就是你用训练样本训练出来的模型，用训练样本来检测的正确率很高，而使用检验样本检验误差很大。
1. 训练误差代表分类方法对于现有训练样本集的拟合程度
2. 泛化误差代表此方法的泛化能力，即对于新的样本数据的分类能力如何
    + 不要过度学习噪声之类的
3. 模型的训练误差比较高，则称此分类模型欠拟合
4. 模型的训练误差低但是泛化误差比较高，则称此分类模型过拟合
5. 对于欠拟合问题，可以通过增加分类属性的数量、选取合适的分类属性等方法，提高模型对于训练样本的拟合程度
6. 最常用的是分为两部分进行对比检测。

## 6.1. 决策树误差曲线
1. 过拟合现象会导致随着决策树的继续增长，尽管训练误差仍在下降，但是泛化误差停止下降，甚至还会提升

![](img\4.png)

1. 横坐标:树的深度或是训练次数
2. 纵坐标:误差
3. 红色曲线:训练样本误差
4. 绿色曲线:测试样本误差
    + 在学习的深度和训练次数的增加过程中，测试样本误差会先下降再上升
    + 所以我们需要在那个关键点处停止学习

## 6.2. 解决过拟合问题
1. 可以使用减少学习的层数来降低过拟合问题。
比如把一开始的树
---
![](img\2.png)

变为
---
![](img\3.png)

2. 可以通过剪枝在相应关键点出停止进行进一步学习。
3. 其他参考方案:
    1. 剪枝方法:保证决策树的复杂程度不断降低。

![](img\5.png)

## 6.3. 剪枝问题
1. 我们需要通过实验，来决定什么时候进行剪枝
    + 一般是从第三层往后会出现过拟合现象。

### 6.3.1. 错误率降低剪枝

![](img\6.png)

### 6.3.2. 悲观剪枝

![](img\7.png)

### 6.3.3. 代价复杂度剪枝策略

![](img\8.png)

# 7. 分类效果评价

![](img\9.png)

1. 具体的其他项目指标:
    1. 查全率:是否查的比较全。查全率和准确率往往相互制约，因为查全率高就容易导致准确率的降低。

![](img\10.png)

2. F值:真正在决策分析时，先看F值，在F值高一点的情况下，再看查全率等，比如看病会牺牲一定的准确率来保证查全率。

![](img\11.png)

## 7.1. 受试者工作曲线(ROC)曲线

1. 受试者工作特征曲线 (ROC)曲线也是一种常 用的综合评价指标。假设检验集中共有20个样本，每个样本为正类或反类，根据分类算法模型可以得出每个样本属于正类的概率，将样本按照此概率由高到低排列。
2. 曲线如下:
    1. 横坐标表示:假正率。
    2. 纵坐标表示:真正率。
    3. 我想要真正率高，假正率低，也就是想要ROC曲线更加高一点比较好。
    4. 做多次实验，可以通过比较曲线下面积来看，一般越高的曲线，噪声会更多。

![](img\12.png)

## 7.2. 提高相应的比率的方法

### 7.2.1. 保留法
保留法将样本集按照定比例划分为训练集与检验集两个集合，两个集合中样本随机分配且不重叠。对于比例的确定，一般情况下，训练集会大于检验集，例如训练集占70%，检验集占30%，具体比例可结合实际情况进行判定。

### 7.2.2. 蒙特卡洛交叉验证(重复随机二次采样验证)
1. 蒙特卡洛交叉验证，也称重复随机二次采样验证，这种验证方法随机将数据集划分为训练集与检验集，使用检验集检验训练集训练的模型效果，多次重复此过程取平均值作为模型好坏的评价标准。蒙特卡洛交叉验证法也 可看作是多次进行保留法。
2. 多次训练，保留比较好的一颗树。

### 7.2.3. K折交叉验证法
1. k折交叉验证法将样本集随机地划分为k个大小相等的子集，在每一轮交叉验证中，选择一个子集作为检验集，其余子集作为训练集，重复k轮，保证每一个子集都作为检验集出现，用K轮检验结果取平均值作为模型好坏的评价标准。最常用的k折交叉验证法为**十折**交叉验证。
2. 每一次，选择一个子集来作为测试，选择其余子集作为训练集，之后取均值，这样就极大的降低了噪声的影响。

### 7.2.4. 留一法
1. 留一法指每次检验集中只包含一个样本的交叉验证方法。

### 7.2.5. 留p法
1. 留p法是每次使用p个样本作为检验集的交叉验证方法

### 7.2.6. 自助法
1. 自助法是统计学中的一种有放回均匀抽样方法，即从一个大小为𝑛的样本数 据集𝑆中构建一个大小为𝑛′的训练样本集St需要进行𝑛′次抽取，每次均可能 抽取到n个样本中的任何一个。𝑛′次抽取之后，剩余的未被抽取到的样本成 为检验集

# 8. 集成学习
1. 引入:多个评委为一个东西进行评选。
2. 集成学习(Ensemble learning)是机器学习中近年来的一大热门领域。其中的集成方法是用多种学习方法的组合来获取比原方法更优的结果。
3. 使用于组合的算法是弱学习算法，即分类正确率仅比随机猜测略高的学习算法，但是组合之后的效果仍可能高于强学习算法，即集成之后的算法准确率和效率都很高

![](img\13.png)

## 8.1. 装袋法
1. 装袋法(Bagging)又称为Bootstrap Aggregating,其原理是通过组合多个训练集 的分类结果来提升分类效果。
2. 装袋法由于多次采样，每个样本被选中的概率相同，因此噪声数据的影响下降，所以装袋法太容易受到过拟合的影响
3. 使用sklearn库实现的决策树装袋法提升分类效果。其中X和Y分别是鸢尾花 （iris）数据集中的自变量（花的特征）和因变量（花的类别）

```python
from sklearn.model_selection import KFold 
from sklearn.model_selection import cross_val_score 
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import datasets #加载iris数据集
iris = datasets.load_iris()
X = iris.data
Y = iris.target
#分类器及交叉验证
seed = 42
kfold = KFold(n_splits=10,random_state=seed)
cart = DecisionTreeClassifier(criterion='gini',max_depth=2)
cart = cart.fit(X, Y)
result = cross_val_score(cart, X, Y, cv=kfold)
print("CART树结果：",result.mean()) model=BaggingClassifier(base_estimator=cart,n_estimators=100, random_state=seed)
result = cross_val_score(model, X, Y, cv=kfold)
print("装袋法提升后结果：",result.mean())
# CART树结果： 0.933333333333 
# 装袋法提升后结果：0.946666666667
```
4. 总结:可以看到装袋法对模型结果有一定提升。当然，提升程度与原模型的结构和数据质量有关。如果分类回归树的树高度设置为3或5，原算法本身的效果就会比较好，装袋法就没有提升空间.

## 8.2. 提升法
1. 提升法(Boosting)与装袋法相比每次的训练样本均为同一组，并且引入了**权重**的概念，给每个单独的训练样本都会分配个相同的初始权重。然后进行T轮训练，每一轮中使用一个分类方法训练出一个分类模型，使用此分类模型对所有样本进行分类并更新所有样本的权重:分类正确的样本权重降低，分类错误的样本权重增加，从而达到更改样本分布的目的。由此可知，每一轮训练后，都会生成一个分类模型，而每次生成的这个分类模型都会更加注意在之前分类错误的样本，从而提高样本分类的准确率。对于新的样本 ，将T轮训练出的T个分类模型得出的预测结果加权平均，即可得出最终的预测结果。
2. 就是权重低的会被加速进行学习。
3. 多轮分类获得多个分类模型
4. 对新样本预测时采用多伦训练得到的分类模型的预测结果的加权平均值。

### 8.2.1. 示例
1. 基于sklearn库中的提升法分类器对决策树进行优化，提高分类准确率。Python代码如下，其中load_breast_cancer()方法加载乳腺癌数据集，自变量 （细胞核的特征）和因变量（良性、恶性）分别赋给X和Y变量。
```python
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score 
from sklearn.ensemble import AdaBoostClassifier 
from sklearn.tree import DecisionTreeClassifier 
from sklearn import datasets 
dataset_all = datasets.load_breast_cancer()
X = dataset_all.data 
Y = dataset_all.target 
seed = 42
kfold = KFold(n_splits=10, random_state=seed) 
dtree = DecisionTreeClassifier(criterion='gini',max_depth=3) 
dtree = dtree.fit(X, Y) 
result = cross_val_score(dtree, X, Y, cv=kfold) 
print("决策树结果：",result.mean()) 
model = AdaBoostClassifier(base_estimator=dtree, n_estimators=100,random_state=seed) 
result = cross_val_score(model, X, Y, cv=kfold) 
print("提升法改进结果：",result.mean())
# 决策树结果： 0.92969924812
# 提升法改进结果：0.970112781955 
```

## 8.3. GBDT(梯度提升决策树)
1. 梯度提升决策树算法是利用梯度下降的思想，使用损失函数的负梯度在当 前模型的值，作为提升树中残差的近似值，以此来拟合回归决策树。梯度提升决策树的算法过程如下：
    1. 初始化决策树，估计一个使损失函数最小化的常数构建一个只有根节点的树。
    2. 不断提升迭代:
        1. 计算当前模型中损失函数的负梯度值，作为残差的估计值;
        2. 估计回归树中叶子节点的区域，拟合残差的近似值;
        3. 利用线性搜索估计叶子节点区域的值，使损失函数极小化;
        4. 更新决策树。
    3. 经过若干轮的提升法迭代过程之后，输出最终的模型
2. 对于GBDT算法的具体实现，最为出色的是XGBoost树提升系统
```python
import pandas as pd
import xgboost as xgb
df = pd.DataFrame({'x':[1,2,3], 'y':[10,20,30]})
X_train = df.drop('y',axis=1)
Y_train = df['y']
T_train_xgb = xgb.DMatrix(X_train, Y_train)
params = {"objective": "reg:linear", "booster":"gblinear"}
gbm = xgb.train(dtrain=T_train_xgb,params=params)
Y_pred = gbm.predict(xgb.DMatrix(pd.DataFrame({'x':[4,5]}))) print(Y_pred)
```
# 9. 随机森林
1. 随机森林是专为决策树分类器设计的集成方式，是装袋法的一种拓展。随机森林与装袋法采取相同的样本抽取方式。装袋法中的决策树每次从所有属性中选取一个最优的属性作为其分支属性，而随机森林算法每次从所有 属性中随机抽取𝑡个属性，然后从这𝑡个属性中选取一个最优的属性作为其分支属性，这样就使得整个模型的随机性更强，从而使模型的泛化能力更强。而对于参数𝑡的选取，决定了模型的随机性，若样本属性共有M个，𝑡=1意味着随机选择一个属性来作为分支属性，𝑡=属性总数时就变成了装袋法集成方式，通常𝑡的取值为小于log2(M + 1)的最大整数。而随机森林算法使用的弱分类决策树通常为CART算法。
2. 类似装袋法的进行抽取，和装袋法不同:
    1. 装袋法每个树的属性是相同的
    2. 随机森林的每个树的属性也可以使不同的，随机程度更强。
3. 实例代码

```python
from sklearn.model_selection import cross_val_score
from sklearn.datasets import make_blobs 
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.tree import DecisionTreeClassifier
X, y = make_blobs(n_samples=1000, n_features=6, centers=50, random_state=0) 
pyplot.scatter(X[:, 0], X[:, 1], c=y) 
pyplot.show()
```
4. <a href = "https://www.cnblogs.com/yszd/p/9583420.html">最终实现</a>


# 10. 决策树总结
1. ID3算法的分支策略是:信息增益。
2. C4.5算法的分支策略是:信息增益率。
3. CART算法的分支策略是:Gini指标。
4. CHARD算法的分支策略是:卡方。
5. 特征有可能是原始的数据，也可能是衍生的数据。决策树是帮忙来寻找特征。

# 11. 分类算法汇总
<a href ="https://mp.weixin.qq.com/s/hN73sdADY--LnUh7Bo9G5Q">分类算法汇总</a>