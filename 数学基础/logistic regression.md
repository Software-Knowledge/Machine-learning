**logistic regression**
1. 逻辑回归属于分类算法的一种。
2. 并且Logistic regression是监督学习问题。

# 1. 回归和分类的区别
1. 连续的数值预测就是回归问题。
2. 离散的类标号预测，就是分类问题。

# 2. 什么是逻辑回归(logistic regression)
1. 逻辑回归时用了回归类似的思路来解决了分类问题。

# 3. 二分类问题
1. 我们假设在一个平面内，要把两组样本点尽可能的分开。
2. 首先:尝试简单的**线性模型**进行分类
    + 问题:噪声对于模型分类的影响是很大的。也就是我们对模型的**鲁棒性**(值控制系统在一定(结构，大小)的参数摄动下，维持其他某些性能的特性)做出了要求。

## 3.1. 广义线性回归
1. Logistics Regression是广义线性回归的一种，广义线性回归主要是为了解决系统中噪声对于这个系统的影响。

### 3.1.1. Logistics Regression的假设函数
![](img\LR\1.png)
1. 这个函数的两个无穷极限分别对应着0和1。
2. 那么这个假设函数的意义是什么呢?
    + 首先，我们对于分类问题，我们使用**极大似然法**(计算所有概率，取最可能的最大值作为我们的预测值)
    + 所以，我想相信你猜到了，假设函数就是所谓的概率，而其范围也是在(0,1)之间。
3. 为什么的新的代价函数是这样子的呢？
    + 因为对于预测失败的情况，通过代价函数应该对假设函数做出惩罚。
    + 而我们要做的最小化代价函数的过程，就是让**惩罚最小**。

### 3.1.2. J函数
1. J函数是如下的
![](img\LR\2.png)
2. 在这个函数之下，其鲁棒性比较好，也就是异常数据虽然会影响，但是依旧能在一定范围内发挥不错的作用。

# 4. 多分类问题
1. 那么如果有很多的属性会影响到最后分类问题。

## 4.1. 假设函数
![](img\LR\3.png)

## 4.2. 代价函数
![](img\LR\4.png)

## 4.3. 计算过程
1. 对于所有的θ分别求导
![](img\LR\5.png)
2. 其中θ<sub>0</sub>是我们添加的恒定为0的
![](img\LR\6.png)
3. 对于一个二分类问题，我们要算1次，而对于n分类的问题，我们要计算n次。

# 5. 参考
1. <a href = "https://blog.csdn.net/guoziqing506/article/details/81328402">逻辑回归(logistic regression)原理详解</a>