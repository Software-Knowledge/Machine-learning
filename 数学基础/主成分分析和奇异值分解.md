**主成分分析和奇异值分解**

# 1. 高维数据数据降维
1. 在机器学习领域的降维，就是采用某种映射方法，将原高维空间中的数据点映射到低维度空间中。
2. 在原始的高维空间中，包含有冗余信息以及噪声信息。噪声过多会造成误差，降低误差准确率；通过降维，减少冗余信息所造成的误差，提高识别的精度。
3. 并且这样子可以提高数据内部的本质结构特征。

## 1.1. 降维的本质

![](img\PCA\1.png)

# 2. 主成分分析(PCA)
1. 主成分分析时最常用的线性降维方法。
    + 丢失原始数据最少
2. 目标:通过某种线性投影，将高维的数据映射到低维的空间上，并期望在所投影的维度上数据的方差最大，以此使用较少的维度，同时保留较多原数据的维度。
    + 方差大:原始数据的丰富度高。
    + 降噪，保留元数据的关系。
3. 如果把所有点都映射到一起，那么几乎所有的区分信息都丢失了，反之，数据点则会分散开，特征更加明显，同时接近原始数据。

## 2.1. 数学化的PCA的目标
1. 求出样本数据的协方差矩阵(散布矩阵)的特征值和特征向量，而矩阵的特征向量的方向就是PCA需要投影的方向。
2. 协方差矩阵和散布矩阵的关系:协方差矩阵乘以(n-1)就是散布矩阵，n是样本数量。并且二者都是对阵矩阵，主对角线是各个随机变量(各个维度)的方差。

## 2.2. PCA的步骤

![](img\PCA\2.png)

1. k是降维的程度。

## 2.3. python实现

![](img\PCA\3.png)

1. 首先生成2个类别共40个3维空间的样本点
2. np.linalg.eig(scatter matrix)直接计算特征值和特征向量。

### 2.3.1. 具体过程

生成类别
---
![](img\PCA\4.png)

计算40个点在3个维度上的平均向量
---
![](img\PCA\5.png)

降低到二维空间
---
![](img\PCA\7.png)
![](img\PCA\6.png)

### 2.3.2. 其他参考

<a href = "https://www.cnblogs.com/clnchanpin/p/7199713.html">详情</a>

## 2.4. 主成分分析特点
1. 利用特征的协方差角度，去找到更加好的**投影方式**。而LDA更多地考虑了标注，即希望投影后不同类别之间数据点的距离更大，同一类别的数据点更紧凑。
2. 当**数据量和数据维度较大**的时候，用协方差矩阵解PCA效率低
3. 解决方法是采用奇异值分解(SVD)

# 3. 奇异值分解(SVD)

![](img\SVD\1.png)

1. 任何矩阵输入我们可以转化成为一个左奇异矩阵和一个右奇异矩阵，中间是一个对角矩阵。
2. 稀疏矩阵用PCA来计算运算量比较大，可以用SVD来进行压缩。

## 3.1. SVD的python实现

![](img\SVD\2.png)

1. 第三行是进行SVD运算
2. 这里略过了数据预处理的过程。

## 3.2. SVD压缩图像

![](img\SVD\3.png)

1. matrix是像素点。
2. k越小压缩的越厉害。所以k=100/200时，已经和原图大小大致差不多，但是已经很清楚。

## 3.3. SVD的python实现

<a href = "https://blog.csdn.net/u012421852/article/details/80439403">详见</a>