梯度下降法
---
1. <a href = "https://blog.csdn.net/qq_29762941/article/details/80343185">详见</a>

# 1. 解正规方程
1. 我们将属性写成一组行向量
    + 对应的是相应的属性的个数

$$x_1 ... x_8$$

1. 我们将属性值表示成一组列向量
    + 对应的是我们计算的结果

$$\theta_1 ... \theta8,\theta_0 == b$$

1. 问题出现:行向量和列向量个数不同，那么我们为行向量加一个x<sub>0</sub> = 0

## 1.1. 假设函数变形

$$x\theta = \sum\theta_ix_i + \theta_0$$

## 1.2. 代价函数变形

$$cost(\theta) = \frac{\sum^m_{j=1}[y_i - x\theta_j]^2}{m}$$

## 1.3. 进一步的变换
1. 我们能不能用一个矩阵来表示所有的数据呢？
    + 可以的，不妨设这个矩阵是X，并且满足x<sub>i,j</sub>表示这是第i组的第j个属性的值。
2. 那么此时对应的假设值应该是

![](img\linear\8.png)

1. 进一步我们把标记也写成矩阵的形式

![](img\linear\9.png)

4. 此时，我们已经可以很明显的猜到了，(Y-Xθ)<sup>T</sup>*(Y-Xθ)来表示这个结果
    + 注:在机器学习中我们通常用一个列向量和其转置矩阵的乘积来表示平方和。
5. 化简后的代价函数:cost(θ) = 1/m * (Y-Xθ)*(Y-Xθ)<sup>T</sup>

## 1.4. 对矩阵求导
结果

![](img\linear\16.png)

## 1.5. 求解最优解
1. 通过矩阵变换得到解θ:

![](img\linear\10.png)

2. 然后我们得到了假设函数:

![](img\linear\11.png)

## 1.6. 注解
1. 有可能正规方程解不出来。

# 2. 批量梯度下降法(BGD)
1. 梯度下降法是一种更加常用于求解假设函数的方法。
2. 为什么叫做批量梯度下降法?    
    + 因为考虑了所有的数据，而且步长一定
    + 问题是:往往会指向一个局部最优解(可以理解成极值)，但是我们想要全局最优解(可以理解成最值)
3. 一定要保证做到**同时更新**

## 2.1. 梯度
1. 梯度:就是一个向量，表示某一函数在该点处的方向导数沿着该方向去的最大值。

![](img\linear\12.png)

2. 我们首先求导，然后按照**解正规方程**的方法的话我们求等于零的解，但是按照**梯度下降法**，我们需要迭代，那么我们就随机取值，然后按照学习步长a(学习速率)来迭代x
    + ![](img\linear\13.png)
    + 通过一次次计算，就会慢慢接近为0的点。
    + **同时更新所有的参数，所以需要先计算出来右侧的式子，然后再更新左侧(不然会做不到不同步)**
3. 梯度过小会导致收敛过慢
4. 梯度多大会可能导致不能收敛
5. 局部最优解问题:如果已经陷入了局部最优解，则不会能进行移动
6. 随着接近最优解:移动的速度会逐渐变小(导数变小)

## 2.2. 求解θ
1. 同样的代价函数也使用梯度的方法
    + ![](img\linear\14.png)

## 2.3. 新的问题来了
1. θ无论如何都不能使得代价函数最小?
    + 我们只要确保小到一定程度即可，比如0.0001

# 3. 随机梯度下降法(SGD)
1. 特点:没有考虑所有的数据，而是仅仅考虑了数据集的一部分完全随机的数据。

# 4. 小批量梯度下降法(MBGD)
1. 特点:每次按照规律的选取数据集，并且不断重复以上过程。

# 5. 三个方法收敛过程的图

![](img\linear\15.png)