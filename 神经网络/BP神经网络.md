**BP神经网络**

# 1. 前馈神经网络(也称作BP神经网络)
1. BP (Back Propagation)神经网络也是前馈神经网络，只是它的参数权重值是由反向传播学习算法进行调整的
2. BP神经网络模型拓扑结构包括**输入层、隐层和输出层**，利用激活函数来实现从输入到输出的任意非线性映射，从而模拟各层神经元之间的交互
    + 全连接神经网路，我们可以证明这个网络足以实现**任意复杂的非线性映射**。
3. 激活函数须满足处处可导的条件。例如，Sigmoid函数连续可微，求导合适，单调递增，输出值是0~1之间的连续量，这些特点使其适合作为神经网络的激活函数

# 2. BP神经网络原理

![](img\BP\1.png)

1. 第2、3层是隐层。
2. 上图中相应网络结构对象的计算公式如下:

![](img\BP\2.png)

2. 式中的𝑤<sub>𝑖𝑗</sub>就是相邻两层神经元之间的权值，它们是在训练过程中需要学习的参数，𝑎1<sup>(2)</sup>中表示第2层的第1个神经元，公式中的(6.1)、(6.2)、(6.3)分别求出了3个神经元的输出结果，而ℎ<sub>𝑊,𝑏</sub>(𝑥)表示第3层第1个神经元 𝑎<sub>1</sub><sup>(3)</sup>的值。图中箭头所指的方向为前向传播的过程，即所有输入参数经过加权求和之后，将结果值依次向下一层传递，直到最后输出层，层数越多、层中神经元越多，形成的权重值参数就越多
3. 要进数值化操作。将性别等数值进行数值化处理即可。

## 2.1. 神经网络训练方式
1. BP神经网络训练过程的基本步骤可以归纳如下
    1. 初始化网络权值和神经元的阈值，一般通过随机的方式进行初始化
    2. 前向传播:计算隐层神经元和输出层神经元的输出
    3. 后向传播:根据目标函数公式修正权值wij
2. 上述过程反复迭代，通过损失函数和成本函数对前向传播结果进行判定，并通过后向传播过程对权重参数进行修正，起到监督学习的作用，一直到满足终止条件为止
3. BP神经网络的核心思想是由后层误差推导前层误差，一层一层的反传，最终获得各层的误差估计，从而得到参数的权重值。由于权值参数的运算量过大，一般采用梯度下降法来实现
4. 所谓梯度下降就是让参数向着梯度的反方向前进一段距离，不断重复，直到梯度接近零时停止。此时，所有的参数恰好达到使损失函数取得最低值的状态，为了避免局部最优，可以采用**随机化梯度**下降

### 2.1.1. 神经网络权重初始化
1. <a href = "https://mp.weixin.qq.com/s/zSFIsOOQV70ZHKpMlFjJvA">详见</a>

## 2.2. 批量处理训练模型
1. 多分类问题为例，第一步向前传播

![](img\BP\3.png)

![](img\BP\4.png)

1. 由于层数较多，因此早期层的梯度变得很小。
2. 批处理算法:一个一个数据组传入进行计算。
    + 将所有的样本都做一遍
3. 我们定义损失函数(代价函数)来进行计算值和真实值之间差异的输出。
    + k是输出的个数。
4. 目标网络输出和最后的结果输出要减小
5. 之后我们需要进行反向计算来调整层与层之间的赋值。
6. 在前馈的时候，变化的是权重和偏置。
    + 层与层之间的权重有着组合。

### 2.2.1. 调整损失函数
1. 目的是求损失函数的极小值
    + 常规:对每一个变量进行求偏导然后等于0联立，但是这计算量过于大。
    + 机器学习领域:我们采用迭代的方式之间接近极小值。
2. 保证速度的情况下找到极小值。(超曲面的波谷)
    + 在当前情况下有一个下降最快的一个点。
    + 这个方向被我们叫做梯度方向，这是上升最快的方向。
3. 当损失函数之间不变化，说明神经网络陷入局部极小值(局部最优解)

### 2.2.2. 权重的修正

![](img\BP\5.png)

1. W<sub>ij</sub>是输入层和隐层
2. W<sub>jk</sub>是隐层和输出层之间
3. 权重是我们用上一次权重减去上次的最大梯度乘以学习步长η
    + η可以是固定值，也可以是动态改变的
    + 步长太短会增加学习次数，步长太长容易确实，步长需要谨慎选择。

### 2.2.3. 总结:一回合包含什么步骤(epoch)
1. 输入所有样本，得到相应输出，然后计算损失函数。
2. 然后我们使用负梯度方向来调整所有的权重以及偏置，当然b也许要进行调整。

![](img\BP\6.png)

3. 知道损失函数接近0到一定程度，或者回合数已经达到之前预设的水平。

## 2.3. 随机梯度下降法
1. 同样输入样本，得到相应输出后，立马进行调整。(对于一个样本)
2. 每次训练都是从n个样本中随机选择一个样本，作为一个epoch。

## 2.4. 小批量处理法(miniepoch)
1. 每次选择十个左右的样本来进行训练。

## 2.5. 问题出现
1. 越接近输入层的梯度会变得非常小，着也就是"梯度消失"
2. 也就是说不能再修正之前权重

## 2.6. 梯度下降算法的正确步骤
1. 初始化随机权重和偏差。
2. 把输入传入网络，得到输出值。
3. 计算预测值和真实值之间的误差。
4. 对每一个产生误差的神经元，改变相应的(权重)值以减少误差。
5. 迭代更新，直到找到最佳权重。

# 3. 径向基函数网络
1. 径向基函数网络的隐含层是由径向基函数神经元组成，这一神经元的变换函数为径向基函数。典型的RBF网络由输入层、RBF隐层和由线性神经元组成的输出层
2. 与传统的即神经网络相比，其主要区别是隐层节点中使用了径向基函数、对输入进行了高斯变换、将在原样本空间中的非线性问题，映射到高维空间中使其变得线性，然后在高维空间里用线性可分算法解决，RBF网络采用高斯函数作为核函数： 
    𝑦 = 𝑒𝑥𝑝[−(𝑏(𝑥 − 𝑤))<sup>2</sup>]
3. RBF网络的隐层神经元自带激活函数，所以其层数可以只有一层隐层，权重值数量更少，所以RBF网络较BP网络速度快很多
4. 目前，RBF神经网络已经成功应用于非线性函数逼近、数据分类、模式识别、图像处理等方向

# 4. 反馈神经网络
1. 与前馈神经网络相比，反馈神经网络内部神经元之间有反馈，可以用一个无向完全图表示，包括了Hopfield网络、BAM网络，Elman网络等
2. Hopfield网络类似人类大脑的记忆原理，即通过关联的方式，将某一件事物与周围场最中的其他事物建立关联，当人们忘记了一部分信息后，可以通过场最信息回忆起来，将缺失的信息找回。通过在反馈神经网络中引入能量函数的概念，使其运行稳定性的判断有了可靠依据，由权重值派生出能量函数是从能量高的位置向能量低的位置转化，稳定点的势能比较低。基于动力学系统理论处理状态的变换，系统的稳定态可用于描述记忆
3. Hopfield网络分为离散型和连续型两种网络
4. 在Hopfield网络中，学习算法是基于Hebb学习规则，权值调整规则为若相邻两个神经元同时处于兴奋状态，那么他们之间的连接应增强，权值增大；反之，则权值减少

## 4.1. 反馈神经网络的训练过程
1. 反馈神经网络的训练过程，主要用于实现记忆的功能，即使用能量的极小点 (吸引子)作为记忆值，一般可应用以下操作来实现训练
    1. 存储:基本的记忆状态，通过权值矩阵存储
    2. 验证:选代验证，直到达到稳定状态
    3. 回忆:没有(失去)记忆的点，都会收敛到稳定的状态
2. 以下是Hopfield网络的一个示例应用，对屏幕点阵模拟的数字进行记忆，经过克罗内克积计算之后，获得了对应数字的参数值矩阵，进行记忆效果评估时，只给出一半的点阵数字信息，通过Hopfield网络进行恢复到原始数字

```python
def kroneckerSquareProduct(self, factor): 
    ksProduct = np.zeros((self.N, self.N), dtype = np.float32)
    for i in xrange(0, self.N): 
        ksProduct[i] = factor[i] * factor
    return ksProduct
#记忆单个数字的状态 
def do_train(self, inputArray): 
    # 归一化
    mean = float(inputArray.sum()) / inputArray.shape[0]
    self.W = self.W + self.kroneckerSquareProduct(inputArray - mean)/(self.N * self.N)/mean/(1-mean)
#通过记忆重构数字
def hopRun(self, inputList):
    inputArray = np.asarray(inputList, dtype = np.float32)
    matrix = np.tile(inputArray, (self.N, 1))
    matrix = self.W * matrix ouputArray = matrix.sum(1)
    # 归一化
    m = float(np.amin(ouputArray))
    M = float(np.amax(ouputArray))
    ouputArray = (ouputArray - m) / (M - m) ouputArray[ouputArray < 0.5] = 0. ouputArray[ouputArray > 0] = 1.
    return np.asarray(ouputArray, dtype = np.uint8)
```

![](img\BP\7.png)

## 4.2. 问题
1. 虽然Hopfield网络具有强大的记忆能力，但是它的缺点也比较明显：
    1. 假记忆问题：只能记住有限个状态，并且当状态之间相似性较大时，或者状态的特征较少或不明显时，容易收敛到别的记忆上。
    2. 存储容量限制：主要依赖极小点的数量。当两个样本距离较近时，就容易产生混淆。假设有n个神经元，那么最多可存0.15n个极小点；能够完美回忆到大部分结果只有𝑛/2𝑙𝑛<sup>𝑛</sup>；完美回忆所有结果只有𝑛/4𝑙𝑛<sup>𝑛</sup>。在上例中，神经元数量是25,则最多只能存储25*0.15=3.75个数字，大约 可回忆的数字为3.88个，能完美回忆的数字为1.9个
    3. 存在局部最优问题

## 4.3. BAM(双向联想记忆神经网络)
1. 双向联想记忆神经网络(BAM)具有非监督学习能力，网络的设计比较简单，可大规模并行处理大量数据，具有较好的实时性和容错性。此外，这种联想记忆法无需对输入向量进行预处理，省去了编码与解码的工作
2. BAM是一种无条件稳定的网络，与Hopfield相比是一种特别的网络，具有输入输出节点，但是Hopfield的不足也一样存在，即存在假记忆、存储容量限制、局部最优等问题

## 4.4. Elman神经网络
1. Elman神经网络是一种循环神经网络，网络中存在环形结构，部分神经元的输出反馈作为输入，而这样的反馈将会出现在该网络的下一个时刻，也即这些神经元在这一时刻的输出结果，反馈回来在下一时刻重新作为输入作用于这些神经元，因此循环神经网络可以有效地应对涉及时序性的问题。

![](img\BP\8.png)

2. 特点:隐层神经元的输出被反馈回来，作为一个单独的结构被定义为承接层，而承接层的数据将与输入层的数据一起作为下一时刻的输入。
3. Elman网络在结构上除了承接层的设置以外，其余部分与BP神经网络的结构大体相同，其隐层神经元通常采用Sigmoid函数作为激活函数，训练过程与BP神经网络相似
4. Elman网络是在时间上动态的，具有内部动态反馈的功能，承接层的设置使得Elman网络能够有效应对具有时变特征的数据，在带有时序性的样本数据上有着比静态神经网络更好的预测性能

## 4.5. 自组织神经网络(Kohonen网)
1. 自组织神经网络又称Kohonen网，这一神经网络的特点是当接收到外界信号刺激时，不同区域对信号自动产生不同的响应。这种神经网络是在生物神经元上首先发现的，如果神经元是同步活跃的则信号加强，如果异步活跃则信号减弱
2. MiniSom库是一种基于Python语言的Numpy实现的自组织映射(SOM)网络。下面是用SOM来实现图片量化，合并不太重要的颜色，减少颜色数量
```python
from minisom import MiniSom 
import numpy as np 
import matplotlib.pyplot as plt 
img = plt.imread('car.jp2')
# 像素转换成矩阵 
pixels = np.reshape(img, (img.shape[0]*img.shape[1], 3)) 
# 像素转换成矩阵 
pixels = np.reshape(img, (img.shape[0]*img.shape[1], 3)) 
# SOM初始化并训练 
som = MiniSom(3, 3, 3, sigma=0.1, learning_rate=0.2) 
# 3x3 = 9 final colors 
som.random_weights_init(pixels) 
starting_weights = som.get_weights().copy()
som.train_random(pixels, 100) 
qnt = som.quantization(pixels) 
clustered = np.zeros(img.shape) 
for i, q in enumerate(qnt): 
    clustered[np.unravel_index(i, dims=(img.shape[0], img.shape[1]))] = q 
    #结果显示 
    plt.figure(1) 
    plt.subplot(221) 
    plt.title('original') 
    plt.imshow(img) 
    plt.subplot(222) 
    plt.title('result') 
    plt.imshow(clustered) 
    plt.subplot(223) 
    plt.title('initial colors') 
    plt.imshow(starting_weights, interpolation='none') 
    plt.subplot(224) 
    plt.title('learned colors') 
    plt.imshow(som.get_weights(), interpolation='none') 
    plt.tight_layout() 
    plt.show()
```

![](img\BP\9.png)